# AI Inference Accelerator
The AI inference accelerator features specialized hardware designed and optimized for inference, the process of producing predictions done by an AI model. It serves to ease, speed up, and lower the cost of AI integration in numerous systems and can be used during either development, deployment, or both. For systems with strong constraints, such as embedded applications, this can be a gamechanger.

Here, we present the design of an AI inference accelerator implemented using open source tools. Using the QSPI (Quad Serial Peripheral Interface), the AI inference accelerator can be easily integrated with a host microcontroller as well as a NAND/NOR flash memory while taking advantage of its relatively high throughput.
